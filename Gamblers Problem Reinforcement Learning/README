README - Thomas Klimek

Dependecies:
	python 3
	matplotlib, numpy
	--> python3 -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose

To run the program simply:
	python3 gambler.py

To implement the value iteration, I used Python 3.6 with the external libraries numpy and matplot lib for storing and plotting the value function and optimal policy. I utilized a Python class to represent the gambler, which can be instantiated specifying probability for heads $(p_h)$, theta threshold for convergence, discount factor, and number of states. This class utilizes member functions update$\_$value to apply the Belman update equation, value$\_$iteration to apply the algorithm, and plot to generate the plots. The value function is represented as an array with the indices corresponding to states, and the values at those indices corresponding to the expected value calculated through the value iteration. The optimal policy is then represented also as an array whose indices correlate to states, and who's values correlate to stakes to place in that state. The optimal policy is selected by choosing the maximum value for each state, and when there is more than one maximum values, the tie is decided by the first occurrence which in our case is the minimal stake. I do however change the way to tie is decided to see other examples of optimal policies generated by the value iteration.

my solution is based of implementations found :
https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Gamblers%20Problem%20Solution.ipynb
https://medium.com/@jaems33/gamblers-problem-b4e91040e58a
https://github.com/prateekbhat91/Gambler-Problem-RL